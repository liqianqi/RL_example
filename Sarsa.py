# import numpy as np
# import gym

# import warnings
# warnings.filterwarnings("ignore", category=DeprecationWarning)

# # è¶…å‚æ•°
# alpha = 0.1       # å­¦ä¹ ç‡
# gamma = 0.99      # æŠ˜æ‰£å› å­
# epsilon = 0.1     # æ¢ç´¢ç‡
# episodes = 500    # è®­ç»ƒå›åˆæ•°

# # åˆ›å»ºç¯å¢ƒ
# env = gym.make('CliffWalking-v0', render_mode="ansi")
# n_states = env.observation_space.n
# n_actions = env.action_space.n

# # åˆå§‹åŒ– Qè¡¨
# Q = np.zeros((n_states, n_actions))

# # è®­ç»ƒ
# for episode in range(episodes):
#     state, _ = env.reset()
#     state = int(state)

#     done = False
#     while not done:
#         # epsilon-è´ªå©ªç­–ç•¥
#         if np.random.rand() < epsilon:
#             action = env.action_space.sample()
#         else:
#             action = np.argmax(Q[state])

#         next_state, reward, terminated, truncated, _ = env.step(action)
#         next_state = int(next_state)
#         done = terminated or truncated

#         # Q-learning æ›´æ–°
#         best_next_action = np.argmax(Q[next_state])
#         Q[state, action] += alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])

#         state = next_state

# print("è®­ç»ƒå®Œæ¯•ï¼âœ…")

# # æµ‹è¯•ï¼šå±•ç¤ºå­¦åˆ°çš„æœ€ä¼˜è·¯å¾„
# state, _ = env.reset()
# state = int(state)
# env.render()

# done = False
# while not done:
#     action = np.argmax(Q[state])
#     next_state, reward, terminated, truncated, _ = env.step(action)
#     next_state = int(next_state)
#     done = terminated or truncated

#     print(env.render())

#     # env.render()

#     state = next_state
# env.close()

import numpy as np
import random
from tqdm import trange

# å®šä¹‰ CliffWalking ç¯å¢ƒï¼ˆ4è¡Œ12åˆ—ï¼‰
class CliffWalkingEnv:
    def __init__(self, ncol=12, nrow=4):
        self.ncol = ncol
        self.nrow = nrow
        self.P = self._create_transition_matrix()

    def _create_transition_matrix(self):
        P = [[[] for _ in range(4)] for _ in range(self.nrow * self.ncol)]
        directions = [[0, -1], [0, 1], [-1, 0], [1, 0]]  # ä¸Šä¸‹å·¦å³

        for i in range(self.nrow):
            for j in range(self.ncol):
                for a in range(4):
                    state = i * self.ncol + j
                    if i == self.nrow - 1 and j > 0:
                        # æ‚¬å´– + ç»ˆç‚¹
                        P[state][a] = [(1.0, state, 0, True)]
                        continue

                    dx, dy = directions[a]
                    next_x = min(self.ncol - 1, max(0, j + dx))
                    next_y = min(self.nrow - 1, max(0, i + dy))
                    next_state = next_y * self.ncol + next_x

                    reward = -1
                    done = False
                    if next_y == self.nrow - 1 and next_x > 0:
                        done = True
                        if next_x != self.ncol - 1:  # æ‰æ‚¬å´–
                            reward = -100

                    P[state][a] = [(1.0, next_state, reward, done)]
        return P

# åˆå§‹åŒ–
env = CliffWalkingEnv()
n_states = env.nrow * env.ncol
n_actions = 4
Q = np.zeros((n_states, n_actions))

# è¶…å‚æ•°
alpha = 0.1
gamma = 0.9
epsilon = 0.1
episodes = 500

# Q-learning ä¸»å¾ªç¯
for _ in trange(episodes):
    state = env.ncol * (env.nrow - 1)  # èµ·ç‚¹ (3, 0)
    done = False

    while not done:
        if random.random() < epsilon:
            action = random.randint(0, n_actions - 1)
        else:
            action = np.argmax(Q[state])

        _, next_state, reward, done = env.P[state][action][0]
        Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])
        state = next_state

print("è®­ç»ƒå®Œæ¯•ï¼âœ…")

# æ˜¾ç¤ºç­–ç•¥å›¾
policy = np.chararray((env.nrow, env.ncol), unicode=True)
actions = ['â†‘', 'â†“', 'â†', 'â†’']
for s in range(n_states):
    x = s % env.ncol
    y = s // env.ncol
    if y == env.nrow - 1 and x > 0:
        policy[y][x] = 'â›”' if x != env.ncol - 1 else 'ğŸ'
    else:
        policy[y][x] = actions[np.argmax(Q[s])]
print("æœ€ä¼˜ç­–ç•¥å›¾ï¼š")
print(policy)
